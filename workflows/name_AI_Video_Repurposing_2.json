{
  "name": "AI Video Repurposing Pipeline (Universal Version)",
  "nodes": [
    {
      "parameters": {},
      "id": "18f99e3a-4dd3-49d6-993d-31728564a51d",
      "name": "Start",
      "type": "n8n-nodes-base.start",
      "typeVersion": 1,
      "position": [
        -20,
        300
      ]
    },
    {
      "parameters": {
        "command": "mkdir -p {{ $json.run_path }}"
      },
      "id": "e331b255-dd3a-44fe-a226-0610313f0175",
      "name": "2. Create Run Folder",
      "type": "n8n-nodes-base.executeCommand",
      "typeVersion": 1,
      "position": [
        400,
        300
      ],
      "executeOnce": true
    },
    {
      "parameters": {
        "command": "=ffmpeg -i {{ $json.run_path }}/video.mp4 -vn -acodec pcm_s16le -ar 16000 -ac 1 {{ $json.run_path }}/audio.wav"
      },
      "id": "c161179e-4a67-4224-bcae-2134b4c6e94a",
      "name": "4. Extract Audio",
      "type": "n8n-nodes-base.executeCommand",
      "typeVersion": 1,
      "position": [
        840,
        300
      ]
    },
    {
      "parameters": {
        "jsCode": "// The LLM returns a JSON string, we need to parse it into a real object\n// that n8n can use, specifically for the Split in Batches node.\nconst aiResponseString = $json.choices[0].message.content;\n\ntry {\n  const parsedJson = JSON.parse(aiResponseString);\n  // The prompt asks for an object { \"clips\": [...] }\n  // We return just the array for the Split in Batches node.\n  return parsedJson.clips;\n} catch (error) {\n  throw new Error(`Failed to parse AI response. The LLM did not return valid JSON. Response was: ${aiResponseString}`);\n}\n"
      },
      "id": "679f22ac-170f-43ea-923f-b25859544715",
      "name": "7. Parse AI Response",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1560,
        300
      ]
    },
    {
      "parameters": {
        "batchSize": 1,
        "options": {}
      },
      "id": "4b68e547-062e-4b46-a010-062972986f3b",
      "name": "8. Loop Through Each Clip",
      "type": "n8n-nodes-base.splitInBatches",
      "typeVersion": 2,
      "position": [
        1780,
        300
      ]
    },
    {
      "parameters": {
        "jsCode": "const fs = require('fs');\n\n// Get the full word-level transcript from the Whisper node\n// We use .$node because this data is from before the Split in Batches loop started\nconst whisperNodeOutput = $items(\"5. Transcribe Audio (Whisper)\")[0].json;\n\n// Check if the expected 'words' property exists\nif (!whisperNodeOutput || !whisperNodeOutput.words) {\n  throw new Error(\"Could not find word-level transcript in the output from the Whisper node. Full output: \" + JSON.stringify(whisperNodeOutput));\n}\n\nconst fullTranscript = whisperNodeOutput.words;\n\n// Get the current clip's info from the loop\nconst clip = $json;\nconst startTime = clip.start_time;\nconst endTime = clip.end_time;\n\n// Filter the transcript to get only the words within this clip's timeframe\nconst clipWords = fullTranscript.filter(word => word.start >= startTime && word.end <= endTime);\n\n// Helper function to format seconds into SRT timestamp format (HH:MM:SS,ms)\nfunction formatSrtTime(seconds) {\n  const date = new Date(0);\n  date.setSeconds(seconds);\n  const timeStr = date.toISOString().substr(11, 12);\n  return timeStr.replace('.', ',');\n}\n\nlet srtContent = '';\n// Create SRT entries in batches of a few words to make captions more readable\nconst wordsPerCaption = 5;\nfor (let i = 0; i < clipWords.length; i += wordsPerCaption) {\n    const chunk = clipWords.slice(i, i + wordsPerCaption);\n    if (chunk.length === 0) continue;\n\n    const srtIndex = (i / wordsPerCaption) + 1;\n    const srtStart = formatSrtTime(chunk[0].start);\n    const srtEnd = formatSrtTime(chunk[chunk.length - 1].end);\n    const srtText = chunk.map(w => w.word).join(' ');\n\n    srtContent += `${srtIndex}\\n${srtStart} --> ${srtEnd}\\n${srtText}\\n\\n`;\n}\n\n// Write the SRT content to a file\n// The file path comes from the node that created the run folder\nconst runPath = $items(\"1. Set Paths & Run ID\")[0].json.run_path;\nconst srtFilePath = `${runPath}/clip_${$loop.iterations}.srt`;\n\nfs.writeFileSync(srtFilePath, srtContent);\n\n// Return the path to the SRT file for the next node\nreturn { srt_file_path: srtFilePath };\n"
      },
      "id": "278e1c6b-67a9-4591-959c-76e3d233c4a2",
      "name": "9. Generate Subtitles File (.srt)",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [
        1780,
        520
      ]
    },
    {
      "parameters": {
        "command": "=ffmpeg -i {{ $items(\"1. Set Paths & Run ID\")[0].json.run_path }}/video.mp4 -ss {{ $json.start_time }} -to {{ $json.end_time }} -vf \"crop=ih*9/16:ih,subtitles={{ $parent.json.srt_file_path }}:force_style='FontName=Arial Black,FontSize=22,PrimaryColour=&HFFFFFF,BorderStyle=3,OutlineColour=&H80000000,Shadow=0,Alignment=10,MarginV=50'\" -c:a copy {{ $items(\"1. Set Paths & Run ID\")[0].json.run_path }}/FINAL_CLIP_{{ $loop.iterations + 1 }}.mp4"
      },
      "id": "e6a1005f-0a13-40e1-b481-9b69b6183353",
      "name": "10. Cut, Reframe & Burn Captions",
      "type": "n8n-nodes-base.executeCommand",
      "typeVersion": 1,
      "position": [
        2020,
        520
      ]
    },
    {
      "parameters": {
        "values": {
          "string": [
            {
              "name": "videoUrl",
              "value": "https://www.youtube.com/watch?v=YOUR_YOUTUBE_VIDEO_URL"
            }
          ]
        },
        "options": {}
      },
      "id": "be66e962-e64e-4b24-b15a-9b4181a4a159",
      "name": "Input Video URL",
      "type": "n8n-nodes-base.set",
      "typeVersion": 2,
      "position": [
        180,
        300
      ]
    },
    {
      "parameters": {
        "values": {
          "string": [
            {
              "name": "run_id",
              "value": "={{ Date.now() }}"
            },
            {
              "name": "run_path",
              "value": "=/home/node/n8n_video_files/{{ $json.run_id }}"
            }
          ]
        },
        "options": {}
      },
      "id": "8f8319e7-49d9-430c-ab2f-e8b9819717bd",
      "name": "1. Set Paths & Run ID",
      "type": "n8n-nodes-base.set",
      "typeVersion": 2,
      "position": [
        180,
        520
      ]
    },
    {
      "parameters": {
        "note": "This workflow re-creates a simplified version of Opus.pro.\n\n**REQUIREMENTS:**\n1.  **n8n in Docker**: This must run in a Docker container.\n2.  **Custom Docker Image**: The n8n container must have `ffmpeg` and `yt-dlp` installed.\n3.  **OpenAI Credentials**: Add your API key to the HTTP Request nodes (5 & 6).\n\n**HOW TO RUN:**\n1.  Replace the URL in the \"Input Video URL\" node.\n2.  Set your OpenAI credentials in nodes 5 and 6.\n3.  Click \"Execute Workflow\"."
      },
      "id": "3bd91d52-5989-49dd-a010-090956b9c9f4",
      "name": "README",
      "type": "n8n-nodes-base.stickyNote",
      "typeVersion": 1,
      "position": [
        -20,
        80
      ],
      "color": "#FFD599"
    },
    {
      "parameters": {
        "command": "=yt-dlp -o \"{{ $json.run_path }}/video.%(ext)s\" -f \"best[ext=mp4]/best\" --no-warnings \"{{ $json.videoUrl }}\""
      },
      "id": "875638c4-11e2-45e5-a043-e3801ec6f8f5",
      "name": "3. Download Video",
      "type": "n8n-nodes-base.executeCommand",
      "typeVersion": 1,
      "position": [
        620,
        300
      ]
    },
    {
      "parameters": {
        "url": "https://api.openai.com/v1/audio/transcriptions",
        "authentication": "headerAuth",
        "sendBody": true,
        "bodyContentType": "multipart-form-data",
        "bodyParameters": {
          "parameters": [
            {
              "name": "model",
              "value": "whisper-1"
            },
            {
              "name": "response_format",
              "value": "verbose_json"
            }
          ]
        },
        "options": {}
      },
      "id": "2785d03a-c859-4592-8889-b8832a826456",
      "name": "5. Transcribe Audio (Whisper)",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.1,
      "position": [
        1080,
        300
      ],
      "credentials": {
        "headerAuth": {
          "id": "YOUR_CREDENTIALS_ID",
          "name": "OpenAI Auth Header"
        }
      }
    },
    {
      "parameters": {
        "url": "https://api.openai.com/v1/chat/completions",
        "authentication": "headerAuth",
        "sendBody": true,
        "bodyContentType": "json",
        "body": "={{ \n{\n    \"model\": \"gpt-4-turbo\",\n    \"response_format\": { \"type\": \"json_object\" },\n    \"messages\": [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are an expert viral video producer for platforms like TikTok and Reels. Your task is to analyze the provided video transcript and identify 5 to 10 engaging, self-contained clips. Each clip must be between 30 and 90 seconds long.\\n\\nFor each identified clip, you must provide:\\n1. `title`: A short, catchy, viral-style title for the clip.\\n2. `start_time`: The precise start time of the clip in seconds (e.g., 123.45).\\n3. `end_time`: The precise end time of the clip in seconds (e.g., 183.45).\\n4. `reason`: A brief explanation of why this segment is compelling and has high viral potential (e.g., addresses a pain point, controversial statement, strong hook).\\n\\nRespond ONLY with a valid JSON object that contains a single key `clips`, which is an array of the clip objects you identified. Do not include any other text or explanation outside of the JSON structure.\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": `Here is the full video transcript. Please analyze it and return the JSON object as instructed.\\n\\nTranscript:\\n${JSON.stringify($input.item.json.text)}`\n        }\n    ]\n}\n}}",
        "options": {}
      },
      "id": "281691a5-83e9-4458-94ed-4f25b30b4279",
      "name": "6. Identify Viral Clips (LLM)",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.1,
      "position": [
        1320,
        300
      ],
      "credentials": {
        "headerAuth": {
          "id": "YOUR_CREDENTIALS_ID",
          "name": "OpenAI Auth Header"
        }
      }
    }
  ],
  "connections": {
    "Start": {
      "main": [
        [
          {
            "node": "Input Video URL",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "2. Create Run Folder": {
      "main": [
        [
          {
            "node": "3. Download Video",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "4. Extract Audio": {
      "main": [
        [
          {
            "node": "5. Transcribe Audio (Whisper)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "7. Parse AI Response": {
      "main": [
        [
          {
            "node": "8. Loop Through Each Clip",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "8. Loop Through Each Clip": {
      "main": [
        [
          {
            "node": "9. Generate Subtitles File (.srt)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "9. Generate Subtitles File (.srt)": {
      "main": [
        [
          {
            "node": "10. Cut, Reframe & Burn Captions",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Input Video URL": {
      "main": [
        [
          {
            "node": "1. Set Paths & Run ID",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "1. Set Paths & Run ID": {
      "main": [
        [
          {
            "node": "2. Create Run Folder",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "3. Download Video": {
      "main": [
        [
          {
            "node": "4. Extract Audio",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "5. Transcribe Audio (Whisper)": {
      "main": [
        [
          {
            "node": "6. Identify Viral Clips (LLM)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "6. Identify Viral Clips (LLM)": {
      "main": [
        [
          {
            "node": "7. Parse AI Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  }
}
